# Synonym_Prediction
利用Bert词向量完成的同义词生成实验

## 概况
本次实验利用Bert生成词向量，并基于一个同义词数据库对词语进行同义词预测。首先，考虑到词语的集合空间是有限的，因此可以利用曼德拉空间距离从20000+个词语样本集合中选出部分词语作为“候选同义词集合”，再利用训练的Siamese Network从中选出正确的同义词。

## 使用方法
### 训练
可以使用 **python siamese_network.py**进行训练，训练完成的模型将会保存在model/中。

### 推理
首先，需要将待生成的文本写入txt文件中，文本中需要替换的词语用{{}}包含，例如这是一个 **{{简单}}的{{文件}}**。之后可以执行 **python final.py --model_path=model/tokenvec_bilstm2_siamese_model.h5 --candidate_num=12 --origin_path=test.txt --save_path=output.txt**。

## 存在的问题
### 精度问题
目前用该方法预测同义词的精度，很大程度上受限于利用曼德拉空间距离筛选出的候选词集合的精度，因此很难有更大的突破，且候选词集合的数量对最终模型准确率的影响非常大————若过小，则利用曼德拉空间距离选取的候选集合可能不包含正确的同义词；若过大，则Siamese Network选择的集合也会更大，更难选中正确的词语。目前经过尝试，candidate_num的值在10-15之间最佳。

通过训练多个模型并进行模型融合，可以达到较高准确率，利用五个模型不同参数下的模型进行stacking可以达到71的准确率，但是多模型融合存在较大的过拟合风险。今后需要考虑将曼德拉空间距离换成一个相对简单的神经网络，来增大候选同义词集合的准确率。

### 时间问题
由于每次推理都需要遍历整个词语集合空间，并对集合中的每个词语调用一次Siamese Network，因此推理速度较慢，一个词语可能需要1-2秒左右的时间。

### 数据集问题
目前使用的数据集相对较小，只收录了20000+条目的同义词，且目前生成的同义词也选自于这20000+个词语。因此，对于不在此数据集中的同义词生成的效果会相对较差。今后需要寻找更大的数据集，并修改生成同义词的搜索空间。
